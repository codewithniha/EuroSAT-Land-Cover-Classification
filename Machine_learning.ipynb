{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63fc41d",
   "metadata": {},
   "source": [
    "# LoopVerse 2025 — EuroSAT Land Cover Classification\n",
    "\n",
    "## Problem Statement: EuroSAT Land Cover Classification with Deep Learning\n",
    "\n",
    "**Background:**\n",
    "Satellite imagery plays a crucial role in environmental monitoring, urban planning, and disaster response. The EuroSAT dataset contains multispectral Sentinel‑2 satellite image tiles of size 64×64 pixels across 10 land-use classes such as Forest, Urban, Water, Agriculture, and others.\n",
    "\n",
    "**Task:**\n",
    "Develop a complete machine learning pipeline that can classify each tile into its correct land-cover type. The dataset is raw, noisy, and inconsistent — requiring cleaning and preprocessing before building the model.\n",
    "\n",
    "**Key Requirements:**\n",
    "1. Data Cleaning and Preprocessing (Handle mixed formats, remove corrupted images, apply filtering)\n",
    "2. Feature Extraction (Color histograms, edges, textures)\n",
    "3. CNN from Scratch (No pretrained models)\n",
    "4. Performance Improvement (Data augmentation, optimization techniques)\n",
    "5. Model Evaluation (Confusion matrix, metrics, CSV predictions)\n",
    "6. Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb5b94",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Imports\n",
    "\n",
    "Import all required libraries including TensorFlow, OpenCV, PIL, scikit-learn, matplotlib, and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63eb36a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opencv-python) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551a73a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (3.11.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.5.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl (375.9 MB)\n",
      "Installing collected packages: tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\admin\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\tensorflow\\\\include\\\\external\\\\com_github_grpc_grpc\\\\src\\\\core\\\\ext\\\\filters\\\\client_channel\\\\lb_policy\\\\grpclb\\\\client_load_reporting_filter.h'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c1587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: tensorflow\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cdccb64",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpip install matplotlib scikit-learn tqdm tensorflow opencv-python pillow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m     sys.exit(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models, callbacks, optimizers\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow.python'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Install missing packages if needed (commented out for local environment)\n",
    "# For local installation: pip install matplotlib scikit-learn tqdm tensorflow opencv-python pillow\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "except ImportError as e:\n",
    "    print(f\"Missing package: {e}\")\n",
    "    print(\"Please install missing packages using:\")\n",
    "    print(\"pip install matplotlib scikit-learn tqdm tensorflow opencv-python pillow\")\n",
    "    sys.exit(1)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb62167",
   "metadata": {},
   "source": [
    "## 2. Configuration and Data Path Setup\n",
    "\n",
    "Define user-configurable variables including DATA_ROOT path, image size, batch size, epochs, random seed, and other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893097b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# User-configurable variables\n",
    "# ------------------------------\n",
    "DATA_ROOT = r\"C:\\Users\\admin\\Downloads\\EuroSAT_RGB (2)\"  # Local path to dataset\n",
    "# Which folder to use for CNN (RGB is typical). If you want to use multispectral change to 'EuroSAT_MS'.\n",
    "DATASET_FOLDER = 'EuroSAT_RGB'  # or 'EuroSAT_MS'\n",
    "IMG_SIZE = (64, 64)\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "RANDOM_SEED = 42\n",
    "MIN_VALID_IMAGES_PER_CLASS = 10\n",
    "CORRUPTED_FOLDER_NAME = 'corrupted'\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Configuration set:\")\n",
    "print(f\"Data Root: {DATA_ROOT}\")\n",
    "print(f\"Dataset Folder: {DATASET_FOLDER}\")\n",
    "print(f\"Image Size: {IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Random Seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a347102",
   "metadata": {},
   "source": [
    "## 3. Image Reading and Corruption Detection Utilities\n",
    "\n",
    "Implement robust image reading functions that handle multiple formats (.jpg, .png, .tif) and corruption detection based on black pixel threshold and variance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path, target_size=IMG_SIZE, channels=CHANNELS):\n",
    "    \"\"\"Read image from path. Handles jpg/png/tif and multi-band tif.\n",
    "    Returns image as uint8 HxWxC or None if unreadable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try Pillow first\n",
    "        img = Image.open(path)\n",
    "        img = img.convert('RGB')  # keep 3 channels\n",
    "        img = img.resize(target_size, Image.BICUBIC)\n",
    "        arr = np.array(img)\n",
    "        if arr.ndim == 2:\n",
    "            arr = np.stack([arr]*3, axis=-1)\n",
    "        if arr.shape[2] < channels:\n",
    "            # pad channels if necessary\n",
    "            pad = channels - arr.shape[2]\n",
    "            arr = np.concatenate([arr] + [arr[..., :1]]*pad, axis=2)\n",
    "        if arr.shape[2] > channels:\n",
    "            arr = arr[..., :channels]\n",
    "        return arr\n",
    "    except Exception:\n",
    "        # fallback to cv2\n",
    "        try:\n",
    "            arr = cv2.imread(str(path), cv2.IMREAD_UNCHANGED)\n",
    "            if arr is None:\n",
    "                return None\n",
    "            # If multi-band (e.g., shape (H, W, >3)), keep first 3\n",
    "            if arr.ndim == 2:\n",
    "                arr = cv2.cvtColor(arr, cv2.COLOR_GRAY2RGB)\n",
    "            elif arr.shape[2] > 3:\n",
    "                arr = arr[:, :, :3]\n",
    "            arr = cv2.resize(arr, target_size, interpolation=cv2.INTER_CUBIC)\n",
    "            arr = cv2.cvtColor(arr, cv2.COLOR_BGR2RGB)\n",
    "            return arr\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "def is_corrupted(img_arr, black_threshold=0.6, zero_var_threshold=1e-6):\n",
    "    \"\"\"Detect corrupted images: too many near-black pixels or very low variance.\"\"\"\n",
    "    if img_arr is None:\n",
    "        return True\n",
    "    # Too many near-zero pixels\n",
    "    gray = cv2.cvtColor(img_arr, cv2.COLOR_RGB2GRAY)\n",
    "    frac_black = np.mean(gray < 5)\n",
    "    if frac_black > black_threshold:\n",
    "        return True\n",
    "    # Extremely low variance\n",
    "    if np.var(img_arr) < zero_var_threshold:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "print(\"Image reading and corruption detection utilities defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601acee2",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning and Preprocessing Pipeline\n",
    "\n",
    "Process the EuroSAT dataset by reading images, detecting and moving corrupted files, applying Gaussian denoising filter, and organizing clean data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data_root, dataset_folder):\n",
    "    \"\"\"Process the EuroSAT dataset with cleaning and preprocessing.\"\"\"\n",
    "    dataset_path = Path(data_root) / dataset_folder\n",
    "    if not dataset_path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset folder not found: {dataset_path}\")\n",
    "\n",
    "    classes = [p.name for p in dataset_path.iterdir() if p.is_dir()]\n",
    "    classes.sort()\n",
    "    print(f\"Found classes ({len(classes)}): {classes}\")\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    image_paths = []\n",
    "    corrupted_dir = dataset_path / CORRUPTED_FOLDER_NAME\n",
    "    corrupted_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for cls in classes:\n",
    "        cls_dir = dataset_path / cls\n",
    "        files = list(cls_dir.glob('*'))\n",
    "        if len(files) == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing {cls}: {len(files)} files found\")\n",
    "        \n",
    "        for f in tqdm(files, desc=f'Processing {cls}', unit='img'):\n",
    "            if f.is_dir():\n",
    "                continue\n",
    "            arr = read_image(f)\n",
    "            if is_corrupted(arr):\n",
    "                # move corrupted to subfolder\n",
    "                try:\n",
    "                    # maintain class subfolder under corrupted\n",
    "                    target = corrupted_dir / cls\n",
    "                    target.mkdir(exist_ok=True)\n",
    "                    shutil.move(str(f), str(target / f.name))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                continue\n",
    "            # Apply denoising filter (Gaussian)\n",
    "            denoised = cv2.GaussianBlur(arr, (3, 3), 0)\n",
    "            X.append(denoised)\n",
    "            y.append(cls)\n",
    "            image_paths.append(str(f))\n",
    "\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y)\n",
    "    print(f\"Loaded images: {len(X)} (corrupted moved to: {corrupted_dir})\")\n",
    "    return X, y, image_paths, classes\n",
    "\n",
    "print(\"Data preprocessing function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb25a00",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction and Visualization\n",
    "\n",
    "Extract color histograms and edge histograms from images, create visualization functions to display sample images with their feature distributions across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_histograms(images, bins=32):\n",
    "    \"\"\"Extract color histograms from images - returns array (N, bins*3)\"\"\"\n",
    "    feats = []\n",
    "    for img in images:\n",
    "        chans = cv2.split(img.astype('uint8'))\n",
    "        hist = np.concatenate([cv2.calcHist([c], [0], None, [bins], [0, 256]).flatten() for c in chans])\n",
    "        hist = hist / np.sum(hist)\n",
    "        feats.append(hist)\n",
    "    return np.array(feats)\n",
    "\n",
    "\n",
    "def extract_edge_histogram(images, bins=32):\n",
    "    \"\"\"Extract edge histograms using Canny edge detection\"\"\"\n",
    "    feats = []\n",
    "    for img in images:\n",
    "        gray = cv2.cvtColor(img.astype('uint8'), cv2.COLOR_RGB2GRAY)\n",
    "        edges = cv2.Canny(gray, 100, 200)\n",
    "        hist = cv2.calcHist([edges], [0], None, [bins], [0, 256]).flatten()\n",
    "        hist = hist / (np.sum(hist) + 1e-8)\n",
    "        feats.append(hist)\n",
    "    return np.array(feats)\n",
    "\n",
    "\n",
    "def visualize_feature_distributions(X, y, classes, n_samples=5):\n",
    "    \"\"\"Show sample images and their histograms (color + edges)\"\"\"\n",
    "    fig, axs = plt.subplots(3, n_samples, figsize=(3*n_samples, 9))\n",
    "    for i in range(n_samples):\n",
    "        idx = np.random.randint(0, len(X))\n",
    "        img = X[idx].astype('uint8')\n",
    "        axs[0, i].imshow(img)\n",
    "        axs[0, i].axis('off')\n",
    "        axs[0, i].set_title(y[idx])\n",
    "\n",
    "        chans = cv2.split(img)\n",
    "        for c_i, c in enumerate(chans):\n",
    "            axs[1, i].plot(cv2.calcHist([c], [0], None, [32], [0,256]).flatten(), alpha=0.7)\n",
    "        axs[1, i].set_title('Color hist')\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        edges = cv2.Canny(gray, 100, 200)\n",
    "        axs[2, i].imshow(edges, cmap='gray')\n",
    "        axs[2, i].axis('off')\n",
    "        axs[2, i].set_title('Edges')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Feature extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e5ef6",
   "metadata": {},
   "source": [
    "## 6. Label Encoding and Data Splitting\n",
    "\n",
    "Convert class names to numerical indices, perform stratified train-validation split, and set up data generators with augmentation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9852734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(y, classes):\n",
    "    \"\"\"Convert class names to numerical indices\"\"\"\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    y_idx = np.array([class_to_idx[cls] for cls in y])\n",
    "    return y_idx, class_to_idx\n",
    "\n",
    "print(\"Label encoding function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec55794",
   "metadata": {},
   "source": [
    "## 7. Baseline CNN Model Architecture\n",
    "\n",
    "Build a CNN from scratch with 3 convolutional blocks, each containing Conv2D, BatchNormalization, ReLU activation, MaxPooling, and Dropout layers, followed by dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c60a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_cnn(input_shape=(64,64,3), num_classes=10):\n",
    "    \"\"\"Build baseline CNN model from scratch\"\"\"\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Conv block 1\n",
    "    model.add(layers.Conv2D(32, (3,3), padding='same', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    # Conv block 2\n",
    "    model.add(layers.Conv2D(64, (3,3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    # Conv block 3\n",
    "    model.add(layers.Conv2D(128, (3,3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Baseline CNN model function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8da81",
   "metadata": {},
   "source": [
    "## 8. Training Helpers and Visualization Functions\n",
    "\n",
    "Functions for plotting training history and visualizing model activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4e32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title_suffix=''):\n",
    "    \"\"\"Plot training and validation accuracy and loss curves\"\"\"\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
    "    axes[0].plot(history.history['loss'], label='train_loss')\n",
    "    axes[0].plot(history.history['val_loss'], label='val_loss')\n",
    "    axes[0].legend(); axes[0].set_title('Loss '+title_suffix)\n",
    "\n",
    "    axes[1].plot(history.history['accuracy'], label='train_acc')\n",
    "    axes[1].plot(history.history['val_accuracy'], label='val_acc')\n",
    "    axes[1].legend(); axes[1].set_title('Accuracy '+title_suffix)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_activations(model, img_tensor, layer_indices=[0,3,6]):\n",
    "    \"\"\"Visualize intermediate layer activations\"\"\"\n",
    "    # img_tensor: 1xHxWxC float32\n",
    "    outputs = [model.layers[i].output for i in layer_indices if i < len(model.layers)]\n",
    "    if not outputs:\n",
    "        print('No layers selected or indices out of range')\n",
    "        return\n",
    "    act_model = models.Model(inputs=model.input, outputs=outputs)\n",
    "    acts = act_model.predict(img_tensor)\n",
    "    for i, act in enumerate(acts):\n",
    "        n_maps = min(act.shape[-1], 6)\n",
    "        fig, axs = plt.subplots(1, n_maps, figsize=(12,2))\n",
    "        for j in range(n_maps):\n",
    "            axs[j].imshow(act[0,:,:,j], cmap='viridis')\n",
    "            axs[j].axis('off')\n",
    "        plt.suptitle(f'Layer {layer_indices[i]} activations')\n",
    "        plt.show()\n",
    "\n",
    "print(\"Training helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723c5e7",
   "metadata": {},
   "source": [
    "## 9. Load and Preprocess Dataset\n",
    "\n",
    "Execute the data loading pipeline and extract features for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca16e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "print(\"Loading and preprocessing EuroSAT dataset...\")\n",
    "X, y, image_paths, classes = prepare_dataset(DATA_ROOT, DATASET_FOLDER)\n",
    "\n",
    "# Normalize to [0,1]\n",
    "X = X / 255.0\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Number of images: {len(X)}\")\n",
    "print(f\"Image shape: {X[0].shape}\")\n",
    "print(f\"Number of classes: {len(classes)}\")\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# Check class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "print(f\"\\nClass distribution:\")\n",
    "for cls, count in class_distribution.items():\n",
    "    print(f\"  {cls}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89842dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction: color hist and edge hist (for analysis)\n",
    "print('Extracting color histograms and edge histograms...')\n",
    "color_feats = extract_color_histograms((X*255).astype('uint8'))\n",
    "edge_feats = extract_edge_histogram((X*255).astype('uint8'))\n",
    "\n",
    "print(f\"Color features shape: {color_feats.shape}\")\n",
    "print(f\"Edge features shape: {edge_feats.shape}\")\n",
    "\n",
    "# Visualize feature distributions\n",
    "print(\"\\nVisualizing sample images and their features...\")\n",
    "visualize_feature_distributions((X*255).astype('uint8'), y, classes, n_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49f106",
   "metadata": {},
   "source": [
    "## 10. Data Splitting and Preparation for Training\n",
    "\n",
    "Split data into training and validation sets with data augmentation setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding and train/validation split\n",
    "y_idx, class_to_idx = encode_labels(y, classes)\n",
    "num_classes = len(classes)\n",
    "\n",
    "print(f\"Class to index mapping: {class_to_idx}\")\n",
    "\n",
    "X_train, X_val, y_train, y_val, paths_train, paths_val = train_test_split(\n",
    "    X, y_idx, image_paths, test_size=0.2, random_state=RANDOM_SEED, stratify=y_idx)\n",
    "\n",
    "print(f'Train samples: {len(X_train)}')\n",
    "print(f'Validation samples: {len(X_val)}')\n",
    "\n",
    "# Data augmentation setup\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    brightness_range=(0.8, 1.2),\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train, \n",
    "    tf.keras.utils.to_categorical(y_train, num_classes), \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "val_generator = val_datagen.flow(\n",
    "    X_val, \n",
    "    tf.keras.utils.to_categorical(y_val, num_classes), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Data generators created with augmentation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da960146",
   "metadata": {},
   "source": [
    "## 11. Baseline Model Training\n",
    "\n",
    "Build, compile, and train the baseline CNN model with callbacks for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build baseline model\n",
    "model = build_baseline_cnn(input_shape=(IMG_SIZE[0], IMG_SIZE[1], CHANNELS), num_classes=num_classes)\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-3), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()\n",
    "\n",
    "# Setup callbacks\n",
    "output_dir = Path(DATA_ROOT).parent  # Save outputs in ML folder\n",
    "checkpoint_path = output_dir / 'best_model_baseline.h5'\n",
    "cb_list = [\n",
    "    callbacks.ModelCheckpoint(str(checkpoint_path), monitor='val_accuracy', \n",
    "                            save_best_only=True, verbose=1),\n",
    "    callbacks.EarlyStopping(monitor='val_loss', patience=6, \n",
    "                          restore_best_weights=True, verbose=1),\n",
    "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, \n",
    "                              patience=3, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"Model created and callbacks configured!\")\n",
    "print(f\"Model will be saved to: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e304221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the baseline model\n",
    "print(\"Starting baseline model training...\")\n",
    "history = model.fit(train_generator, \n",
    "                   epochs=EPOCHS, \n",
    "                   validation_data=val_generator, \n",
    "                   callbacks=cb_list)\n",
    "\n",
    "print(\"Baseline model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75aa708",
   "metadata": {},
   "source": [
    "## 12. Training History Visualization\n",
    "\n",
    "Plot training and validation accuracy and loss curves to analyze model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for baseline model\n",
    "plot_history(history, title_suffix='(Baseline)')\n",
    "\n",
    "# Print final metrics\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "print(f\"\\nBaseline Model Final Metrics:\")\n",
    "print(f\"Training Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Validation Loss: {final_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c037bb",
   "metadata": {},
   "source": [
    "## 13. Baseline Model Evaluation and Metrics\n",
    "\n",
    "Generate predictions, create confusion matrix, and save results to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1edada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model on validation set\n",
    "val_preds = model.predict(X_val)\n",
    "y_pred = np.argmax(val_preds, axis=1)\n",
    "\n",
    "print('Baseline Model Classification Report:')\n",
    "print(classification_report(y_val, y_pred, target_names=classes))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(f'\\nConfusion Matrix Shape: {cm.shape}')\n",
    "print('Confusion matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Save predictions to CSV\n",
    "df = pd.DataFrame({'image_id': paths_val, 'predicted_label': [classes[i] for i in y_pred]})\n",
    "csv_out = output_dir / 'predictions_baseline.csv'\n",
    "df.to_csv(csv_out, index=False)\n",
    "print(f'\\nSaved baseline predictions to: {csv_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad760e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save confusion matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Baseline Model (Validation Set)')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save confusion matrix plot\n",
    "cm_path = output_dir / 'confusion_matrix_baseline.png'\n",
    "plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confusion matrix saved to: {cm_path}\")\n",
    "\n",
    "# Save baseline model\n",
    "model_path = output_dir / 'loopverse_baseline_model.h5'\n",
    "model.save(model_path)\n",
    "print(f'Baseline model saved to: {model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e1e41",
   "metadata": {},
   "source": [
    "## 14. Improved CNN Model Implementation\n",
    "\n",
    "Build an enhanced CNN architecture with deeper layers, adjusted dropout rates, and learning rate scheduling for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building improved model (deeper + different dropout + LR schedule)')\n",
    "\n",
    "def build_improved_cnn(input_shape, num_classes):\n",
    "    \"\"\"Build improved CNN with deeper architecture and tuned parameters\"\"\"\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # First conv block (deeper)\n",
    "    x = layers.Conv2D(32, (3,3), padding='same')(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(32, (3,3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Second conv block (deeper)\n",
    "    x = layers.Conv2D(64, (3,3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(64, (3,3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Third conv block\n",
    "    x = layers.Conv2D(128, (3,3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.Dropout(0.35)(x)\n",
    "\n",
    "    # Dense layers\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs=inp, outputs=out)\n",
    "\n",
    "# Build improved model\n",
    "improved = build_improved_cnn((IMG_SIZE[0], IMG_SIZE[1], CHANNELS), num_classes)\n",
    "improved.compile(optimizer=optimizers.Adam(learning_rate=1e-3), \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "improved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a23be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule callback\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch > 0 and epoch % 8 == 0:\n",
    "        return lr * 0.5\n",
    "    return lr\n",
    "\n",
    "lr_cb = callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "# Setup callbacks for improved model\n",
    "checkpoint_path2 = output_dir / 'best_model_improved.h5'\n",
    "cb2 = [\n",
    "    callbacks.ModelCheckpoint(str(checkpoint_path2), monitor='val_accuracy', \n",
    "                            save_best_only=True, verbose=1),\n",
    "    callbacks.EarlyStopping(monitor='val_loss', patience=8, \n",
    "                          restore_best_weights=True, verbose=1),\n",
    "    lr_cb\n",
    "]\n",
    "\n",
    "# Train improved model\n",
    "print(\"Starting improved model training...\")\n",
    "history2 = improved.fit(train_generator, \n",
    "                       epochs=EPOCHS, \n",
    "                       validation_data=val_generator, \n",
    "                       callbacks=cb2)\n",
    "\n",
    "print(\"Improved model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot improved model training history\n",
    "plot_history(history2, title_suffix='(Improved)')\n",
    "\n",
    "# Print final metrics for improved model\n",
    "final_train_acc2 = history2.history['accuracy'][-1]\n",
    "final_val_acc2 = history2.history['val_accuracy'][-1]\n",
    "final_train_loss2 = history2.history['loss'][-1]\n",
    "final_val_loss2 = history2.history['val_loss'][-1]\n",
    "\n",
    "print(f\"\\nImproved Model Final Metrics:\")\n",
    "print(f\"Training Accuracy: {final_train_acc2:.4f}\")\n",
    "print(f\"Validation Accuracy: {final_val_acc2:.4f}\")\n",
    "print(f\"Training Loss: {final_train_loss2:.4f}\")\n",
    "print(f\"Validation Loss: {final_val_loss2:.4f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "print(f\"\\nComparison with Baseline:\")\n",
    "print(f\"Validation Accuracy Improvement: {final_val_acc2 - final_val_acc:.4f}\")\n",
    "print(f\"Validation Loss Improvement: {final_val_loss - final_val_loss2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bec85f",
   "metadata": {},
   "source": [
    "## 15. Performance Comparison and Analysis\n",
    "\n",
    "Compare baseline and improved models, evaluate metrics, and visualize confusion matrices side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate improved model\n",
    "val_preds2 = improved.predict(X_val)\n",
    "y_pred2 = np.argmax(val_preds2, axis=1)\n",
    "\n",
    "print('Improved Model Classification Report:')\n",
    "print(classification_report(y_val, y_pred2, target_names=classes))\n",
    "\n",
    "# Confusion matrix for improved model\n",
    "cm2 = confusion_matrix(y_val, y_pred2)\n",
    "\n",
    "# Plot confusion matrices side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Baseline confusion matrix\n",
    "axes[0].imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "axes[0].set_title('Confusion Matrix - Baseline Model')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "tick_marks = np.arange(len(classes))\n",
    "axes[0].set_xticks(tick_marks)\n",
    "axes[0].set_xticklabels(classes, rotation=45)\n",
    "axes[0].set_yticks(tick_marks)\n",
    "axes[0].set_yticklabels(classes)\n",
    "\n",
    "# Improved confusion matrix\n",
    "im = axes[1].imshow(cm2, interpolation='nearest', cmap='Blues')\n",
    "axes[1].set_title('Confusion Matrix - Improved Model')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "axes[1].set_xticks(tick_marks)\n",
    "axes[1].set_xticklabels(classes, rotation=45)\n",
    "axes[1].set_yticks(tick_marks)\n",
    "axes[1].set_yticklabels(classes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.colorbar(im, ax=axes.ravel().tolist())\n",
    "\n",
    "# Save comparison plot\n",
    "comparison_path = output_dir / 'confusion_matrix_comparison.png'\n",
    "plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confusion matrix comparison saved to: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44027805",
   "metadata": {},
   "source": [
    "## 16. Activation Visualization (Optional)\n",
    "\n",
    "Visualize intermediate layer activations to understand what the CNN has learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activations from a sample image\n",
    "if len(X_val) > 0:\n",
    "    # Select a random validation image\n",
    "    sample_idx = np.random.randint(0, len(X_val))\n",
    "    sample_img = X_val[sample_idx:sample_idx+1]  # Keep batch dimension\n",
    "    sample_class = classes[y_val[sample_idx]]\n",
    "    \n",
    "    print(f\"Visualizing activations for sample image (True class: {sample_class})\")\n",
    "    \n",
    "    # Show the original image\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(sample_img[0])\n",
    "    plt.title(f'Sample Image - True Class: {sample_class}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize activations from different layers of the improved model\n",
    "    # Layer indices to visualize (conv layers)\n",
    "    layer_indices = [0, 3, 6, 9]  # First conv layers from each block\n",
    "    visualize_activations(improved, sample_img, layer_indices)\n",
    "else:\n",
    "    print(\"No validation data available for activation visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d016e",
   "metadata": {},
   "source": [
    "## 17. Results Export and Model Saving\n",
    "\n",
    "Save trained models, export prediction CSV files, and organize all outputs for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceabba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save improved model predictions\n",
    "df2 = pd.DataFrame({'image_id': paths_val, 'predicted_label': [classes[i] for i in y_pred2]})\n",
    "csv_out2 = output_dir / 'predictions_improved.csv'\n",
    "df2.to_csv(csv_out2, index=False)\n",
    "print(f'Saved improved predictions to: {csv_out2}')\n",
    "\n",
    "# Save improved model\n",
    "model_path2 = output_dir / 'loopverse_improved_model.h5'\n",
    "improved.save(model_path2)\n",
    "print(f'Improved model saved to: {model_path2}')\n",
    "\n",
    "# Save individual improved confusion matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(cm2, interpolation='nearest', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Improved Model (Validation Set)')\n",
    "plt.colorbar()\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "\n",
    "cm2_path = output_dir / 'confusion_matrix_improved.png'\n",
    "plt.savefig(cm2_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Improved confusion matrix saved to: {cm2_path}\")\n",
    "\n",
    "# Summary of all generated files\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFiles created in {output_dir}:\")\n",
    "print(f\"📊 predictions_baseline.csv\")\n",
    "print(f\"📊 predictions_improved.csv\")\n",
    "print(f\"🖼️  confusion_matrix_baseline.png\")\n",
    "print(f\"🖼️  confusion_matrix_improved.png\")\n",
    "print(f\"🖼️  confusion_matrix_comparison.png\")\n",
    "print(f\"🤖 loopverse_baseline_model.h5\")\n",
    "print(f\"🤖 loopverse_improved_model.h5\")\n",
    "print(f\"🤖 best_model_baseline.h5\")\n",
    "print(f\"🤖 best_model_improved.h5\")\n",
    "\n",
    "print(f\"\\n📈 Final Results Summary:\")\n",
    "print(f\"Baseline Model Validation Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"Improved Model Validation Accuracy: {final_val_acc2:.4f}\")\n",
    "print(f\"Improvement: {final_val_acc2 - final_val_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Ready for LoopVerse 2025 submission!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
